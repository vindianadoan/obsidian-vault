## 2.1. What is Statistical Learning?

- We assume there's a relationship between $Y$ and $X = (X_1, X_2, \dots, X_p)$ which can be represented in the general form:
$$Y = f(x) + \epsilon$$
- Where $f$ is a fixed but unknown function of $X_1, \dots, X_p$, and $\epsilon$ is a random error term (independent of $X$ and mean 0). $f$ represents the systematic information that $X$ provides about $Y$, and is something we need to estimate based on observed points.
- Statistical learning refers to the set of approaches for estimating $f$

### 2.1.1 Why Estimate f?

- Estimating $f$ can be useful for both prediction (predicting the output with a known set of inputs - prediction accuracy is paramount here) and inference (learning the relationship between the inputs and the output - interpretability is the priority).
- There is often a trade-off between prediction power and interpretability (e.g. more complex models may provide better predictions, but are harder to interpret)

### 2.1.2 How Do We Estimate f?

- Techniques for estimating f can be characterised as either parametric or non-parametric.
- Parametric techniques require that a functional form be assumed for the data, which provides a set of parameters that one needs to estimate using the data. i.e. it reduces the problem of estimating f down to one of estimating a set of parameters.
	- in general, fitting a more flexible parametric model involves fitting more parameters
- Non-parametric techniques do not make assumptions about the functional form of 